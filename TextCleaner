from sklearn.base import BaseEstimator, TransformerMixin
import re
import string
from contractions import contractions_dict
import nltk
nltk.download('stopwords')
from nltk.corpus import wordnet , stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
ps = PorterStemmer()


class TextCleaner(BaseEstimator, TransformerMixin):

    def __init__(self, text_features):
        self.text_features=text_features
        
    @staticmethod
    def expand_contractions(text):
        """
        This function expand the given text eg: couldn't -> could not
        Parameters
        ----------
        text : str
                input string
        Returns
        -------
        str
            expanded string
        """
        pattern = re.compile(
            "({})".format("|".join(contractions_dict.keys())),
            flags=re.DOTALL | re.IGNORECASE)

    def replace_text(t):
        
        txt = t.group(0)
        if txt.lower() in contractions_dict.keys():
            return contractions_dict[txt.lower()]

        expand_text = pattern.sub(replace_text, text)
        return expand_text
    
    
    def clean_text(text):
        '''
        This function cleans the text 

        Parameters
        ------------
        text: str
            input string to be cleaned

        Returns
        ------------
        str
            clean text
        '''
        text = text.lower() # lowercase text
        # removing urls
        text = re.sub(r"http\S+", "", text)
        # Fixes contractions such as `you're` to you `are`
        text = self.expand_contractions(text)
        # White spaces removal
        text = text.strip()
        # removing numbers
        text = re.sub(r'\d+', '', text)
        # Punctuation removal
        text = text.translate(str.maketrans('', '', string.punctuation))
        # remove stopwords from text
        text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
        # "lemmatization" & "stemming" 
        text = word_tokenize(text)
    #     text = [lemmatizer.lemmatize(word) for word in text]
    #     text = [ps.stem(word) for word in text]
        text = ps.stem(lemmatizer.lemmatize(text))
        clean_text = ' '.join(text)

        clean_text = re.sub(r"\s([?.,!'{}])".format('"'), r'\1', clean_text)
        return clean_text.strip()

        
    def fit(self, X=None, y=None):
        """
        fits data frame column wise
        Parameters
        ----------
        X : pd.DataFrame
            Input samples.
        y :  pd.Series
            Target values
        Returns
        -------
        self
        """
        return self

    def transform(self, X=None, y=None):
        """
        performs column wise text cleaning
        Parameters
        ----------
        X : pd.DataFrame
            Input samples.
        y :  pd.Series
            Target values
        Returns
        -------
        pd.DataFrame
            Transformed data frame
        """
        for col in self.text_features:
            X[col] = X[col].apply(clean_text)
        X.drop(self.text_features, axis=1)
        return X

    def fit_transform(self, X, y=None, **fit_params):
        """
        Fit to data, then transform it.
        Fits transformer to `X` and `y` with optional parameters `fit_params`
        and returns a transformed version of `X`.
        Parameters
        ----------
        X : pd.DataFrame
            Input samples.
        y :  pd.Series
            Target values
        **fit_params : dict
            Additional fit parameters.
        Returns
        -------
        pd.DataFrame
            Transformed data frame.
        """
        _ = self.fit(X=X, y=y)
        return self.transform(X=X, y=y)
